<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bohan Chen&#39;s Personal Webpage</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Bohan Chen&#39;s Personal Webpage</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 28 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu5000416120042106492.png</url>
      <title>Bohan Chen&#39;s Personal Webpage</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>üéâ Caltech SURF 2026</title>
      <link>http://localhost:1313/post/surf2026/</link>
      <pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/surf2026/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#research-focus&#34;&gt;Research Focus&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-machine-learning-for-data-assimilation&#34;&gt;1. Machine Learning for Data Assimilation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-machine-learning-theory-attention-mechanisms-and-transformers&#34;&gt;2. Machine Learning Theory: Attention Mechanisms and Transformers&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#expectations-for-students&#34;&gt;Expectations for Students&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references-for-surf&#34;&gt;References for SURF&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Summer Undergraduate Research Fellowships (SURF) program is one of the &amp;ldquo;crown jewels&amp;rdquo; of Caltech. Since 1979, SURF students have had the opportunity to conduct research under the guidance of experienced mentors working at the frontier of their fields.&lt;/p&gt;
&lt;p&gt;In 2026, I will serve again as a mentor in the SURF program.&lt;/p&gt;
&lt;p&gt;This year, there will be &lt;strong&gt;two main research directions&lt;/strong&gt; available:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning for Data Assimilation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning Theory, with a focus on Attention Mechanisms and Transformers&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may indicate your preferred direction in your application; I am happy to discuss which option is a better fit for your background and interests.&lt;/p&gt;
&lt;h2 id=&#34;research-focus&#34;&gt;Research Focus&lt;/h2&gt;
&lt;h3 id=&#34;1-machine-learning-for-data-assimilation&#34;&gt;1. Machine Learning for Data Assimilation&lt;/h3&gt;
&lt;p&gt;This direction will explore accessible yet meaningful problems in &lt;strong&gt;data assimilation (DA)&lt;/strong&gt;, with an emphasis on how &lt;strong&gt;machine learning methods&lt;/strong&gt; can enhance or complement classical DA techniques.&lt;/p&gt;
&lt;p&gt;Possible topics include (but are not limited to):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing &lt;strong&gt;ML-augmented ensemble filters&lt;/strong&gt; that are &lt;em&gt;amortized&lt;/em&gt; over families of observation operators and noise levels, so that a single learned component can adapt efficiently to changing observation models and uncertainty structures.&lt;/li&gt;
&lt;li&gt;Using &lt;strong&gt;machine learning to approximate optimal particle filters&lt;/strong&gt;, with learned mechanisms that jointly improve both the &lt;strong&gt;prediction&lt;/strong&gt; (propagation) step and the &lt;strong&gt;analysis&lt;/strong&gt; (update) step, aiming for better stability, accuracy, and computational efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The project is designed to be manageable. I will provide detailed guidance, including theoretical background in both DA and ML, as well as relevant code resources. A typical workflow may include reading and discussing selected papers, implementing algorithms in Python, running controlled numerical experiments, and analyzing the results.&lt;/p&gt;
&lt;p&gt;My hope is to collaborate closely with the students to produce a research paper by the end of the project. However, if our results are not substantial enough for a publication, that is completely fine. My primary goal is for you to gain a solid understanding and valuable skills in DA and ML.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-machine-learning-theory-attention-mechanisms-and-transformers&#34;&gt;2. Machine Learning Theory: Attention Mechanisms and Transformers&lt;/h3&gt;
&lt;p&gt;This direction will use &lt;strong&gt;carefully designed experiments to explore questions in modern machine learning theory&lt;/strong&gt;, with a particular focus on the role of the &lt;strong&gt;activation function inside the attention mechanism&lt;/strong&gt; of Transformer architectures.&lt;/p&gt;
&lt;p&gt;According to my current theoretical work, under certain assumptions, the conventional choice of softmax in attention may &lt;strong&gt;not&lt;/strong&gt; be theoretically optimal. Motivated by this, I would like SURF students to systematically investigate, through experiments, how replacing softmax with alternative activation functions changes the behavior and performance of Transformers.&lt;/p&gt;
&lt;p&gt;In this project, you may:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement Transformer variants in which the attention weights are produced by &lt;strong&gt;different activation functions&lt;/strong&gt; (for example, polynomial activations, ReLU-type alternatives, or normalization-based replacements for softmax).&lt;/li&gt;
&lt;li&gt;Design and run experiments on &lt;strong&gt;benchmark tasks&lt;/strong&gt; to compare these variants in terms of accuracy, stability, and training dynamics.&lt;/li&gt;
&lt;li&gt;Probe how the &lt;strong&gt;optimization, generalization, and robustness&lt;/strong&gt; of these models relate to the theoretical predictions that I am developing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is already a growing empirical literature on &lt;strong&gt;softmax-free or softmax-alternative attention mechanisms&lt;/strong&gt;. This project is designed to complement those works from a &lt;strong&gt;theory-first perspective&lt;/strong&gt;: I will start from theoretical analysis to derive concrete hypotheses, and you will help test these hypotheses experimentally. Through this process, you will both:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gain a &lt;strong&gt;deeper understanding of the theory behind Transformers and attention&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;Build &lt;strong&gt;practical experience&lt;/strong&gt; implementing and training modern Transformer-style models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;expectations-for-students&#34;&gt;Expectations for Students&lt;/h2&gt;
&lt;p&gt;My primary expectation is that you bring &lt;strong&gt;genuine enthusiasm&lt;/strong&gt; to this project. Please make sure that at least one of the two general research directions outlined above aligns with your interests and that you are willing to dedicate time and effort to it over the summer of 2026.&lt;/p&gt;
&lt;p&gt;In terms of skills, it would be very helpful if you have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A foundational understanding of &lt;strong&gt;probability&lt;/strong&gt;, &lt;strong&gt;stochastic processes&lt;/strong&gt;, &lt;strong&gt;(ordinary) differential equations&lt;/strong&gt;, and &lt;strong&gt;linear algebra&lt;/strong&gt; (ideally from coursework).&lt;/li&gt;
&lt;li&gt;Solid &lt;strong&gt;coding proficiency&lt;/strong&gt;, especially in &lt;strong&gt;Python&lt;/strong&gt;; experience with &lt;strong&gt;PyTorch&lt;/strong&gt; (or a similar framework) is helpful but not strictly required‚ÄîI am happy to provide guidance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Passion and curiosity are the key qualities I am looking for. If you are excited about these topics and ready to engage deeply, I believe you will find this project both rewarding and educational.&lt;/p&gt;
&lt;h2 id=&#34;references-for-surf&#34;&gt;References for SURF&lt;/h2&gt;
&lt;p&gt;For more information on the SURF program, please refer to the official 
. This page provides additional links to details about the program, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;I would like to extend my heartfelt thanks to my supervisor, Professor Andrew Stuart, for his invaluable support in my role as a SURF mentor. His guidance and encouragement have been instrumental in enabling me to participate in this program and share these opportunities with undergraduate students.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2024-present 
.&lt;/p&gt;
&lt;p&gt;Released under the 
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk Announcement: 1W-MINDS Seminar ‚Äî Learning Enhanced Ensemble Filters</title>
      <link>http://localhost:1313/event/1w-minds-seminar/</link>
      <pubDate>Sat, 01 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/1w-minds-seminar/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#event-details&#34;&gt;Event Details&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#talk&#34;&gt;Talk&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#presenter&#34;&gt;Presenter&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#add-to-calendar&#34;&gt;Add to Calendar&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;I‚Äôm pleased to share that I‚Äôll be speaking in the &lt;strong&gt;1W-MINDS&lt;/strong&gt; seminar series. The talk will present new results on learning-augmented ensemble filtering for nonlinear, non-Gaussian state estimation.&lt;/p&gt;
&lt;h2 id=&#34;event-details&#34;&gt;Event Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; Thursday, &lt;strong&gt;November 6, 2025&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; &lt;strong&gt;2:30 PM ‚Äì 3:30 PM Eastern&lt;/strong&gt; (11:30 AM ‚Äì 12:30 PM Pacific)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Format:&lt;/strong&gt; Online (Zoom)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zoom:&lt;/strong&gt; 

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; 101 &lt;em&gt;(likely not required when using the full link)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;talk&#34;&gt;Talk&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Learning Enhanced Ensemble Filters&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;br&gt;
The filtering distribution in hidden Markov models evolves according to a mean-field law on the joint state‚Äìobservation space. While the ensemble Kalman filter (EnKF) provides a robust particle approximation, its Gaussian ansatz limits accuracy for non-Gaussian dynamics. We address this with a measure neural mapping (MNM)‚Äîa neural operator defined on probability measures‚Äîwhich yields the MNM-enhanced ensemble filter (MNMEF) in both the mean-field limit and as an interacting-particle algorithm. The ensemble instantiation uses a permutation-invariant set transformer, enabling a single parameterization to operate across varying ensemble sizes, with light size-specific fine-tuning further improving accuracy. Empirically, MNMEF achieves superior RMSE to leading methods on Lorenz-96 and Kuramoto‚ÄìSivashinsky benchmarks.&lt;/p&gt;
&lt;p&gt;A central theoretical contribution establishes a continuum limit for attention on measures: attention layers fed empirical measures are consistent with their continuous-measure counterparts, with outputs converging to those from the underlying measure in Wasserstein distance as the sample size goes to infinity. This result justifies cross-size parameter sharing, explains the observed stability of MNMEF when changing ensemble sizes.&lt;/p&gt;
&lt;h2 id=&#34;presenter&#34;&gt;Presenter&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bohan Chen, California Institute of Technology, U.S.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;add-to-calendar&#34;&gt;Add to Calendar&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Eastern Time:&lt;/strong&gt; Nov 6, 2025, 2:30‚Äì3:30 PM ET&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pacific Time:&lt;/strong&gt; Nov 6, 2025, 11:30 AM‚Äì12:30 PM PT&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Flow Matching for Efficient and Scalable Data Assimilation</title>
      <link>http://localhost:1313/publication/transue-flow-2025/</link>
      <pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/transue-flow-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Enhanced Ensemble Filters</title>
      <link>http://localhost:1313/publication/bach-learning-2025/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/bach-learning-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk Announcement: Learning Enhanced Ensemble Filters at SOCAMS 2025</title>
      <link>http://localhost:1313/event/southern-california-applied-mathematics-symposium-socams/</link>
      <pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/southern-california-applied-mathematics-symposium-socams/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#talk-details&#34;&gt;Talk Details&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#venue--schedule&#34;&gt;Venue &amp;amp; Schedule&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;I‚Äôm delighted to present my current work on data assimilation at the Southern California Applied Mathematics Symposium (SOCAMS) 2025. SOCAMS brings together researchers and students in applied mathematics for a day of talks, posters, and networking at UC¬†Riverside.&lt;/p&gt;
&lt;h2 id=&#34;talk-details&#34;&gt;Talk Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Title:&lt;/strong&gt; Learning Enhanced Ensemble Filters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Presenter:&lt;/strong&gt; Bohan Chen&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time Slot:&lt;/strong&gt; 16:15‚Äì16:35 on Saturday, April 26, 2025&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;venue--schedule&#34;&gt;Venue &amp;amp; Schedule&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt; UC Riverside campus, centered around the Student Success Center and the Rivera (Library) Walkway&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full Schedule:&lt;/strong&gt; 
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Venue Map:&lt;/strong&gt; 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Talk Announcement: SIAM DS25 Minisymposium ‚ÄòAdvances in Dynamic Data Analysis‚Äô</title>
      <link>http://localhost:1313/event/siam-ds25/</link>
      <pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/siam-ds25/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#session-details&#34;&gt;Session Details&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#my-talk-schedule&#34;&gt;My Talk Schedule&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;I‚Äôm pleased to announce that I will be presenting in the minisymposium ‚ÄúAdvances in Dynamic Data Analysis: Transfer Operators, Machine Learning, and Networks¬†‚Äì Part¬†II of¬†II‚Äù at the SIAM Conference on Applications of Dynamical Systems (DS25).&lt;/p&gt;
&lt;h2 id=&#34;session-details&#34;&gt;Session Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; Monday, May¬†12,¬†2025&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Session:&lt;/strong&gt; MS87 ‚Äì Advances in Dynamic Data Analysis: Transfer Operators, Machine Learning, and Networks¬†‚Äì Part¬†II of¬†II&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; 4:45¬†PM¬†‚Äì¬†6:45¬†PM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Room:&lt;/strong&gt; Plaza Concourse Level ‚Äì Plaza Ballroom¬†BCEF&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;See Part¬†I:&lt;/strong&gt; MS69&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;my-talk-schedule&#34;&gt;My Talk Schedule&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;5:45¬†‚Äì¬†6:10¬†PM&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Learning Enhanced Ensemble Filters&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Bohan Chen, California Institute of Technology, U.S.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GLL: A Differentiable Graph Learning Layer for Neural Networks</title>
      <link>http://localhost:1313/publication/brown-gll-2024/</link>
      <pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/brown-gll-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The CommUnity near-Surface Permafrost (CUSP) dataset a global compilation of permafrost observations and related properties to support AI/ML model development</title>
      <link>http://localhost:1313/publication/solander-cusp-2024/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/solander-cusp-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>üéâ Caltech SURF 2025</title>
      <link>http://localhost:1313/post/surf2025/</link>
      <pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/surf2025/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#i-have-already-selected-my-surf-student&#34;&gt;I have already selected my SURF student&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#research-focus&#34;&gt;Research Focus&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#expectations-for-students&#34;&gt;Expectations for Students&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;i-have-already-selected-my-surf-student&#34;&gt;I have already selected my SURF student&lt;/h2&gt;
&lt;p&gt;This is an update on 2/11/2025 that I have already selected my 2025 SURF student. I am writing the SURF proposal with him. Thank you very much for your interest in my research and the SURF program. However, I sincerely apologize as I am unable to accommodate any further inquiries at this time.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Summer Undergraduate Research Fellowships (SURF) program is one of the &amp;ldquo;crown jewels&amp;rdquo; of Caltech. Since 1979, SURF students have had the opportunity to conduct research under the guidance of experienced mentors working at the frontier of their fields.&lt;/p&gt;
&lt;p&gt;In 2025, I will serve as a mentor in the SURF program, co-mentoring alongside CMS postdoctoral researcher 
. Together, we will guide a team of 2 to 3 undergraduate students.&lt;/p&gt;
&lt;h2 id=&#34;research-focus&#34;&gt;Research Focus&lt;/h2&gt;
&lt;p&gt;The research project in the SURF program will address accessible yet meaningful problems in data assimilation (DA), particularly in combining DA with machine learning methods. The project is designed to be manageable, and I will provide detailed guidance, including theoretical background in both DA and ML, as well as relevant code resources.&lt;/p&gt;
&lt;p&gt;My hope is to collaborate closely with the students to produce a research paper by the end of the project. However, if our results are not substantial enough for a publication, that is completely fine. My primary goal is for students to gain a solid understanding and valuable skills in DA and ML. Ultimately, if you walk away from this project having learned a great deal, I will consider it a success.&lt;/p&gt;
&lt;h2 id=&#34;expectations-for-students&#34;&gt;Expectations for Students&lt;/h2&gt;
&lt;p&gt;My primary expectation is that you bring genuine enthusiasm to this project. Please make sure that the general research direction outlined above aligns with your interests and that you‚Äôre willing to dedicate time and effort to it over the summer of 2025.&lt;/p&gt;
&lt;p&gt;In terms of skills, I hope you have a foundational understanding of probability, stochastic processes, (ordinary) differential equations, and linear algebra‚Äîideally having taken coursework in these areas. Coding proficiency is also important; Python experience is essential, and familiarity with or experience in PyTorch is a plus, though not required‚ÄîI‚Äôm happy to provide guidance on this if needed.&lt;/p&gt;
&lt;p&gt;Passion and curiosity are the key qualities I‚Äôm looking for. If you‚Äôre excited about this topic and ready to engage deeply, I believe you‚Äôll find this project both rewarding and educational.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;For more information on the SURF program, please refer to the official 
. This page provides additional links to details about the program, including 
, 
, 
, and 
.&lt;/p&gt;
&lt;p&gt;For a comprehensive understanding of data assimilation, I highly recommend the following resource:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inverse Problems and Data Assimilation: A Machine Learning Approach&lt;/strong&gt;: available on 
.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;I would like to extend my heartfelt thanks to my supervisor, Professor Andrew Stuart, for his invaluable support in my role as a SURF mentor. His guidance and encouragement have been instrumental in enabling me to participate in this program and share this opportunity with undergraduate students.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2024-present 
.&lt;/p&gt;
&lt;p&gt;Released under the 
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Personal Icon Design</title>
      <link>http://localhost:1313/post/my-personal-icon/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/my-personal-icon/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#use-and-purpose&#34;&gt;Use and Purpose&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#copyright&#34;&gt;Copyright&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This personal icon is a unique design created independently by me, Bohan Chen, inspired by symbols from mathematics. The concept combines the symbols ‚àë (Sigma), œÄ (Pi), and ‚à´ (Integral) into one cohesive form. In my design, the Sigma symbol transforms seamlessly into the shape of an integral sign, which cleverly intertwines to resemble the shape of Pi. This integration of mathematical symbols not only represents my affinity for mathematics but also serves as a distinctive representation of my identity, Bohan Chen.&lt;/p&gt;
&lt;h2 id=&#34;use-and-purpose&#34;&gt;Use and Purpose&lt;/h2&gt;
&lt;p&gt;This icon is currently used as the symbol for my personal website and will also be incorporated into other platforms and contexts as a recognizable personal mark that represents me, Bohan Chen.&lt;/p&gt;
&lt;h2 id=&#34;copyright&#34;&gt;Copyright&lt;/h2&gt;
&lt;p&gt;Copyright 2024-present 
.&lt;/p&gt;
&lt;p&gt;All rights reserved. This icon is an original design and is protected under copyright law. It may not be copied, reproduced, or redistributed in any form without explicit permission from the copyright holder. For inquiries regarding use or licensing, please contact me directly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models</title>
      <link>http://localhost:1313/publication/chapman-narrative-2024/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chapman-narrative-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CGAP: A Hybrid Contrastive and Graph-based Active Learning Pipeline to Detect Water and Sediment in Multispectral Images</title>
      <link>http://localhost:1313/publication/chen-cgap-nodate/</link>
      <pubDate>Fri, 07 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-cgap-nodate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Batch Active Learning for Multispectral and Hyperspectral Image Segmentation Using Similarity Graphs</title>
      <link>http://localhost:1313/publication/chen-batch-2024/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-batch-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AutoKG: Efficient Automated Knowledge Graph Generation for Language Models</title>
      <link>http://localhost:1313/publication/chen-autokg-2023/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-autokg-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>http://localhost:1313/experience/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Material Identification in Complex Environments: Neural Network Approaches to Hyperspectral Image Analysis</title>
      <link>http://localhost:1313/publication/brown-material-2023/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/brown-material-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph-Based Active Learning for Nearly Blind Hyperspectral Unmixing</title>
      <link>http://localhost:1313/publication/chen-graph-based-2023/</link>
      <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-graph-based-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph-Based Active Learning for Surface Water and Sediment Detection in Multispectral Images</title>
      <link>http://localhost:1313/publication/chen-graph-based-2023-1/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-graph-based-2023-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Novel batch active learning approach and its application to synthetic aperture radar datasets</title>
      <link>http://localhost:1313/publication/chapman-novel-2023/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chapman-novel-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Novel Point Process Model for COVID-19: Multivariate Recursive Hawkes Process</title>
      <link>http://localhost:1313/publication/chen-novel-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-novel-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling illegal logging in Brazil</title>
      <link>http://localhost:1313/publication/chen-modeling-2021/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/chen-modeling-2021/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
